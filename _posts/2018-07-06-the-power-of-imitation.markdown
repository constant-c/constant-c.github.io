---
layout: post
title:  "The Power of Imitation"
date:   2018-07-06 08:00:00 +0200
permalink: /imitation-learning/
categories: Imitation Learning
---

One of the greatest challenges in [Reinforcement Learning][RL] (**RL**) is the exploration of the state-action space. 
Consider a RL agent starting with a blank slate in some state. 
It doesn't know anything about the environment it is in, and every possible action appears to be equally good (or bad).
All the agent can do, is to pick a random action and hope for the best. 
Let's consider further, the agent has learned some good policy after a while through painstaking trail and error. 
How can we be sure that there is no better policy out there - a policy that is more rewarding?
That is the **exploration problem**. 
The problem of having to try every possible action in every possible state to arrive at an optimal policy.

At this point, you are probably thinking: 
*"Hey, why do we need to start with a blank slate? 
Why can't I incorporate my knowledge of the environment in the agent's learning phase?
I could demonstrate how to behave in the environment, and the agent learns to act as I do."*
That is called **Imitation Learning** (**IL**) and yes, you can. 

In the following, I will briefly sketch out Imitation Learning and it's pitfalls. 
I will demonstrate the advantage of the simple **DAgger** IL algorithm over plain Behavioural Cloning with support from Berkley's Deep RL course ([homepage][rl_course], [github][rl_repo]).

# Behavioural Cloning
In IL, we have access to an expert policy $$\pi^{exp}$$ - a mapping of states to actions - during learning. 
Often, that expert policy is denoted $$\pi^*$$ indicating that the expert is optimal. 
But I feel, that an expert is not necessarily optimal.
Think of an human expert. A human is good - but hardly optimal on many tasks. So, we will call it $$\pi^{exp}$$.
Irregardless of the naming, this expert policy may be generated by a human controller, some rule-based system or a "classic" control method.
All we need to do now, is cloning - the highest form of imitating - the expert's behaviour, right? 

Well, we could use the expert by executing (or rolling out) $$\pi^{exp}$$ in the environment and record each state and the action the expert picked in it.
And if we do that a few times, we will end up with a nice data set of states and corresponding actions. 
And if we know a little about supervised machine learning, we could then fit a nice function approximator, such as a neural network, on that.
The state vectors would be our features, and the actions our labels. 
So, by the time our expert hits the environment, it already knows what to do, since it got a clone of the expert's behaviour $$\pi^{exp}$$.
Could work! But, before I tell you what the problem with this is, let us talk about DAgger.

# Nail Your Data with DAgger  
**DAgger** stands for Dataset Aggregation and stems from a paper published in 2011 by [Ross et al][dagger_paper].
It is one of the most popular IL algorithms out there, partly because the algorithm is so straight forward.
Basically, DAgger works by:
1. executing a mixture of expert and agent policy in each roll out;
2. collecting all visited states;
3. using the expert's policy $$\pi^{exp}$$ to label all states;
4. aggregating the latest data with prior data;
5. fitting a new agent policy to the entire data set.

Here is some pseudo-code:
{% highlight python %}
def dagger(expert_policy, num_rollouts):
    # Initialize 
    data_set = {}
    novice_policy = build_network()

    # Run DAgger
    for i in range(num_rollouts):
    
        # Build policy
        def mixed_policy(s):
            if beta(i) >= random(0, 1):     # beta(i) goes to 0 as i goes to infinity
                return expert_policy(s)
            else:
                return novice_policy.predict(s)
                
        new_data = roll out mixed_policy in environment
        new_data['actions'] = expert_policy(new_data['states'])

        # Aggregate data
        data_set = union(data_set, new_data)

        # Update the novice policy
        novice_policy.fit(data_set['states'], data_set['actions'])

    return novice_policy
{% endhighlight%}  

That looks pretty much like the Behavioural Cloning, that we discussed previously, maybe with the exception that it is online! 
So, why should that work any better?

# The Distribution Mismatch Problem
The greatest advantage of imitation learning is often also it's biggest pitfall. 
Do you remember, how we wanted to help the agent in the beginning, by incorporating our own knowledge of what is good into the learning process?
Well, by doing that, we are essentially limiting the state space to a small region, that we believe is "good".
The problem now is, if the agent happens to be outside that state region, it probably wouldn't know what to do.
That is the **distribution mismatch problem**. 
It means, the distribution the agent was trained on, does not match necessarily the distribution it is trying to make predictions on.  

Let me illustrate that with an example. 
Given our agent is a beer-fetching robot, we could demonstrate (and teach to the robot), how to pick a beer bottle from the fridge.
But what happens if the robot accidentally picks up a bottle of milk (god forbid!)? 
Would it bring us the milk and utterly fail it's task? Or would it place the milk back and pick the beer instead?
If we used Behavioural Cloning to train the robots policy, and only demonstrated picking beer bottles, it would come back to us with milk.
Or do some other crazy thing.

The beauty of DAgger is, the algorithm allows the agent to make mistakes and pick the wrong action.
And then shows, what the expert would have done in that state.
That makes the novice policy a lot more robust compared to Behavioural Cloning.
In other words, every time the agent wanders off into "not so good" state space regions, the expert is pointing it back.

# Some Evidence to Back All of This Up
To demonstrate, how much better an Imitation Learning algorithm like DAgger can perform, compared to Behavioural Cloning, let us try both in an experiment.
We pick as test case the Humanoid environment of the [OpenAI Gym][gym]. 
The goal of this benchmark problem is to get a humanoid figure to walk or run as fast as possible by controlling it's 17 joints simultaneously.

We try to approximate the expert policy from [Berkley's DeepRL repo][rl_repo] with a densely connected neural network, that has 2 hidden layers of 256 units each.
The cost function is the Mean Squared Error between expert and novice policy, and we choose ADAM as optimizer.

For Behavioural Cloning, we generated roughly 500k samples by executing $$\pi^{exp}$$ for 500 times. 
The neural network is fitted in 5 epochs over the whole data set. 
We then roll out the learned novice policy for another 120 times to evaluate it's performance.

Similarly, we roll out DAgger for 120 times and train the novice policy over 5 epochs in each roll out. 
$$\beta = 0.95^i$$ in our experiments, where $$i$$ is the (zero-indexed) number of iterations.

![IL Results](/assets/il_comparision.png)

*Results from the OpenAI Gym environment "Humanoid-v2". Both, DAgger and Behavioural Cloning (BC), share the same architecture of 2 hidden layers of 256 units each. DAgger was trained on 92k samples, BC on 496k.*

We can see in the figure, that there is a slight dip in the performance of DAgger early on in the learning, when the share of $$\pi^{exp}$$ in the mixed policy is reduced.
But most astonishing is, that DAgger reaches the realms of the expert's performance, while Behavioural Cloning does not. 
And that is, although, DAgger is trained on 1/5 of the numbers of samples that are available to the BC algorithm. 
Indeed, we can see the power of Imitation Learning in mitigating the Exploration and Distribution Mismatch problems.
  
[RL]: "https://www.constantin.ai/intro-to-rl"
[rl_course]: "http://rll.berkeley.edu/deeprlcoursesp17/"
[rl_repo]: "https://github.com/berkeleydeeprlcourse/homework"
[dagger_paper]: "https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf"
[gym]: "http://gym.openai.com/"